---
layout: fullscreen
title: "Two Stage Reason"
---

<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Take A Step Back: Rethinking the Two Stages in
    Visual Reasoning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://mybearyzhang.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://mybearyzhang.github.io/projects/TwoStageReason">
            Two-Stage Visual Reasoning
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Take A Step Back: Rethinking the Two Stages in
            Visual Reasoning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://mybearyzhang.github.io">Mingyu Zhang</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://Caijiting.github.io">Jiting Cai</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://mingyulau.github.io/">Mingyu Liu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://silicx.github.io/">Yue Xu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.mvig.org">Cewu Lu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://dirtyharrylyl.github.io/">Yong-Lu Li</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Shanghai Jiao Tong University,</span>
            <span class="author-block"><sup>2</sup>Zhejiang University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/todo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/todo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/mybearyZhang/TwoStageReason"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/images/general_final.pdf"
                type="image/pdf">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/general_final.png" alt="General Final" width="100%">
      <h2 class="subtitle has-text-centered">
        Our two-stage framework follows <span class="dnerf">Separated-Encoder-Shared-Reasoner</span> design to reach generalization.
      </h2>
    </div>
  </div>
</section>




<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Visual reasoning, as a prominent research area, plays a crucial role in AI by facilitating concept formation and interaction with the world.
            However, current works are usually carried out separately on small datasets thus lacking generalization ability.
          </p>
          <p>
            Through rigorous evaluation of diverse benchmarks, we demonstrate the shortcomings of existing ad-hoc methods in achieving cross-domain reasoning and their tendency to data bias fitting.
            In this paper, we revisit visual reasoning with a two-stage perspective: (1) symbolization and (2) logical reasoning given symbols or their representations. 
            We find that the reasoning stage is better at generalization than symbolization.
            Thus, it is more efficient to implement symbolization via <b>separated</b> encoders for different data domains while using a <b>shared</b> reasoner.
          </p>
          <p>
            Given our findings, we establish design principles for visual reasoning frameworks following the separated symbolization and shared reasoning.
            The proposed two-stage framework achieves impressive generalization ability on various visual reasoning tasks, including puzzles, physical prediction, and visual question answering (VQA), encompassing both 2D and 3D modalities.
            We believe our insights will pave the way for generalizable visual reasoning.
          </p>
        </div>
        <img src="./static/images/teaser_long.png" alt="General Final" width="100%">
        <h6> Comparison between end-to-end model, human beings, and our framework. A specific end-to-end model is needed for each task, while our framework would share a logical reasoner similar to human intelligence. </h6>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Our Two-Stage Perspective</h2>
        <div class="content has-text-justified">
          <p>
            As described above, visual reasoning can be divided into two stages: the symbolization stage extracts symbolic representations of the underlying data, and the reasoning stage performs logical reasoning.
            For humans, different modalities of visual and auditory information collected from our sensors are converted into electrical signals through different pathways and then sent to the cerebellar cortex to perform logical reasoning.
            Analogously, separated task-specific symbolizers and a shared domain-independent reasoner would be a reasonable choice for a general visual reasoning machine.
            Besides, the reasoner should be capable of performing unified reasoning on input information from various modalities. In other words, the essence of reasoning lies in its generalization ability.
          </p>
          <p>
            <b>Symbolization Stage.</b> During the stage of symbolization, we implement various task-oriented feature extraction networks. These networks employ symbol encoders tailored for each task, transforming multi-modal inputs (text, image, video) into symbol representations. Formally, suppose we have n tasks.
            For the i-th task, we have the input data xi and the task ti, and the task-oriented encoder \(E^i\). Then we get the symbol representation set \(f^i\) via:
            \[f^i = E_i(x^i | t^i)\]
          </p>
          <p>
            <b>Reasoning Stage.</b> The reasoner is fed by symbolic representations for each specific task, in a bid to capture a deeper and more comprehensive understanding of the underlying patterns and relationships embedded within the data.
            For symbol representation sets \(\{f^{(i)}\}_n^{i=1}\) of all tasks, we send them into the reasoner \(R\), and get its reasoning result set \(\{c^i\}_n^{i=1}\) after the logic processing to facilitate problem-solving across various modalities:
            \[\{c^i\}_n^{i=1} = R(\{f^i\}_n^{i=1}).\]
          </p>
          <p>
            <b>Task-specific Heads.</b> The final part of our framework is the task-specific heads, which take the reasoning results from the reasoner as input and generate task-specific answers. For different tasks, we need to construct task-specific classification or regression heads \(H^i\) to get the final output \(s^i\). That says:
            \[s^i = H^i(c^i | t^i).\]
          </p>
        </div>
        <img src="./static/images/share.png" alt="General Final" width="100%">
        <h6> 4 Types of disentanglement arrangements of encoders and reasoners. Through experiments, we demonstrate that Type 4: Separated-Encoder-Shared-Reasoner is the most effective. </h6>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">LLM-based Models Reasoning</h2>
        <div class="content has-text-justified">
          <p>
            To analyze the performance of LLM-based models,
            we probe the task according to our two-stage framework design and examine them separately:
            (1) <b>Symbolization</b>: whether LLM-based models can recognize the elements of the problem.
            (2) <b>Conceptual</b>: whether LLM-based models can learn specific concepts behind the tasks and reason about them.
            (3) <b>Answer Generation</b>: whether LLM-based models can utilize the concepts it learns to solve problems.
            Using MiniGPT-4 as a representative,
            we summarize the typical responses of LLM-based models to three-level problems in RAVEN and Bongard.
          </p>
          <p>
            We find that LLMs may encounter certain hallucination circumstances while solving visual reasoning tasks. As shown in the figure below, for the RAVEN problem, MiniGPT-4 succeeds in the first level of identifying the object while failing in the second stage of reasoning with the arrangement rule.
            For RAVEN problems, MiniGPT-4 fails to accurately identify the logical patterns.
            For the Bongard problem, MiniGPT-4 succeeds in the first level of recognizing human activity and the second level of grasping reasoning logically, yet it fails at the answer generation level and gets lost when utilizing rules to answer 
            questions.
            Given the above cases, we can gain an understanding of the shortcomings of the LLM-based models in reasoning tasks, namely its good concept comprehension ability but insufficient performance in logical reasoning and answer generation. 
          </p>
        </div>
        <img src="./static/images/LLM_picture.png" alt="General Final" width="100%">
        <h6> Failure case analysis of LLM-based model. On RAVEN, it fails on the Symbolization level; while on Bongard-HOI, it fails on the Answer Generation level. </h6>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2024take,
  author    = {Zhang, Mingyu and Cai, Jiting and Liu, Mingyu and Xu, Yue and Lu, Cewu and Li, Yong-Lu},
  title     = {Take A Step Back: Rethinking the Two Stages in Visual Reasoning},
  journal   = {ECCV},
  year      = {2024},
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/mybearyzhang" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
